{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Creating Spark Session and Loading the Data\n",
    "\n",
    "###  Importing Libraries : \n",
    "\n",
    "#### 1) Spark Context :  \n",
    "Spark Context is the entry point to the services of Apache Spark.This library is imported to initialize spark.\n",
    "\n",
    "#### 2) Spark Session : \n",
    "Importing Sparksession to allow programming Spark with DataFrame and DataSets.\n",
    "\n",
    "#### 3) SQL Functions\n",
    "\n",
    "Importing SQL functions such as mean(),when(),col() etc.\n",
    "\n",
    "\n",
    "#### 3) Feature Transformation : \n",
    "\n",
    "Importing feature transforemers  and estimators , such as  StringIndexer,OneHotEncdoer and VectorAssembler from pyspark.ml.features module.\n",
    "\n",
    "#### 4) MultiClassificationEvaluator : \n",
    "\n",
    "MultiClassificationEvaluator  is used to calculate the accuracy. It accepts two columns , the \n",
    "raw Predictions and the label. \n",
    "\n",
    "#### 5) Classification Algorithms:\n",
    "\n",
    "The Libraries DecisionTreeClasifier, RandomForestClassifier ,GBTClassifier,LogisticRegression are imported\n",
    "from the pyspark.ml.classification for the Classification Algorithms.\n",
    "\n",
    "\n",
    "#### 6) Matplotlib : \n",
    "\n",
    "The library Matplotlib is imported to plot the Bar Graph for the Accuracies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier,GBTClassifier,LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics,BinaryClassificationMetrics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Step 01: Import ​ pyspark ​ and initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1 Code :\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Using the getOrCreate() function to create a new spark context or to get an existing Spark Context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):  # Initializing an if loop to Check whether there is an existing Spark Context or not.\n",
    "    # using the SparkContext() function to create a new spark context if there is no existing Spark Context\n",
    "    # local[4] is running Spark Locally with 4 working processors.\n",
    "    # The app name is  the name that is shown on the Spark Clustering UI\n",
    "    sc = SparkContext(master=\"local[4]\", appName=\"Assignment 2\")\n",
    "    \n",
    "    \n",
    "#initializing a SparkSession with sparkContext = sc\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 02: Load the dataset and print the schema and total number of entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the data frame Data Frame**\n",
    "\n",
    "The data frame is loaded with  the spark.read.csv() function. The parameter header = True  includes the heading. The parameter inferSchema  = True is used to correctly represent the datatypes of the Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_data = spark.read.csv('weatherAUS.csv',header = True,inferSchema = True)  # Reading a Spark data frame\n",
    "weather_data.printSchema()  # using printSchema() function to print the schema of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of entries in the DataFrame : \n",
    "\n",
    "The number of entries in the dataframe can be foundout using the count() function, The count() function\n",
    "returns the number of rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total  number of entries in the data are :  142193\n"
     ]
    }
   ],
   "source": [
    "print(\"The total  number of entries in the data are : \",weather_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Cleaning and Processing\n",
    "\n",
    "\n",
    "\n",
    "###  Step 03: Delete columns from the dataset\n",
    "\n",
    "\n",
    "#### Methodology : \n",
    "A list is defined inorder to store the columns to  be dropped. The spark.dataframe.select() method is used to select all the columns that are not in the dropped columns list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Delete useless data\n",
    "# Creating a list of useless columns\n",
    "drop_list = ['Date', 'Location', 'Evaporation', 'Cloud9am','Cloud3pm','Temp9am','Temp3pm','Sunshine']\n",
    "# Selecting the columns that are not present in the dropped columns list \n",
    "weather_data = weather_data.select([column for column in weather_data.columns if column not in drop_list])\n",
    "\n",
    "# Printing the Schema using printSchema() after dropping the columns\n",
    "weather_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04: Print the number of missing data in each column.\n",
    "\n",
    "The missing data can be foundout by selecting a subdataframe of all the rows containing the string \"NA\" as\n",
    "the missing data is represented by the string NA.The .count() function is used to count the number of rows \n",
    "contaaining the string **NA** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in column MinTemp are :  637\n",
      "Number of nulls in column MaxTemp are :  322\n",
      "Number of nulls in column Rainfall are :  1406\n",
      "Number of nulls in column WindGustDir are :  9330\n",
      "Number of nulls in column WindGustSpeed are :  9270\n",
      "Number of nulls in column WindDir9am are :  10013\n",
      "Number of nulls in column WindDir3pm are :  3778\n",
      "Number of nulls in column WindSpeed9am are :  1348\n",
      "Number of nulls in column WindSpeed3pm are :  2630\n",
      "Number of nulls in column Humidity9am are :  1774\n",
      "Number of nulls in column Humidity3pm are :  3610\n",
      "Number of nulls in column Pressure9am are :  14014\n",
      "Number of nulls in column Pressure3pm are :  13981\n",
      "Number of nulls in column RainToday are :  1406\n",
      "Number of nulls in column RainTomorrow are :  0\n"
     ]
    }
   ],
   "source": [
    "#  initializing a for loop to iterate through columns \n",
    "for column in weather_data.columns:\n",
    "    # Selecting Null Values by equating each column to NA \n",
    "    # Using the .count() function to count the number of entries\n",
    "    Null_values = weather_data[weather_data[column] == \"NA\"].count()\n",
    "    # Printing the number of values by using typecasting for the NullValues as they are integer type\n",
    "    print(\"Number of nulls in column \" + str(column) +  \" are :  \" + str(Null_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 05: Fill the missing data with average value and maximum occurrence value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values for Numerical Data with Mean : \n",
    "\n",
    "\n",
    "#### Methodology : \n",
    "A list of all the numeric columns is selected . The missing values  is imputed iteratively for each element in the list i.e. for each column in the list. The mean imputation is used for the Numeric columns, the mean is computed using the mean() function from the pyspark sql functions and then the .collect() method is used to collect the row object as a list and the first element, and the mean object is selected by indexing [0]['mean']. \n",
    "\n",
    "The when() function is used to implement the condition i.e. when the column value is NA, the NA value is replaced  with the mean. .otherwise() function is used to represent the non Missing values. The .withColumn() function is used to perform specific operations on a column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of Numerical Columns : \n",
    "numerical_columns = ['MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm']\n",
    "for i in numerical_columns: # Initializing a for loop to iterate through the list of Numerical values\n",
    "    # Imputing the missing values with mean using mean() function\n",
    "    # When() function is used for the conditional imputation, when the column values are NA the mean is used to \n",
    "    # impute the column values.    \n",
    "\n",
    "    weather_data = weather_data.withColumn(i,when(col(i) == \"NA\",str(weather_data.select(mean(col(i)).alias('mean')).collect()[0]['mean'])).otherwise(col(i)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_data.printSchema() # Printing the schema of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values for Non Numeric columns with Mode ( Most Repeated Value).\n",
    "\n",
    "The missing values in the non numeric columns are imputed with the mode, i.e. the most frequently occuring value.\n",
    "\n",
    "All the non numeric columns are taken in a list and the imputation is done iteratively for each column in the list.The counts of all the values in the column can be foundout by the .groupB().count() function.The orderBy() function, is used to order the columns in either ascending or descending order.The columns are ordered by the desc('count') i.e. in the decreasing order, the most repeated value on the top.The .collect() method is used to collect the Row Objects into a list and the the element at the 0th index of the  specific column i.e. the most frequent value of the column is selected and is replaced.\n",
    "\n",
    "The .withColoumn() function is used to make modifications on a specific column.The when() function is used for conditional imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the non numeric columns in a list\n",
    "non_numeric_columns = [x for x in weather_data.columns if x not in numerical_columns]\n",
    "for i in non_numeric_columns: # for loop to iterate through the elements of the list\n",
    "    # replacing the NA values with the Mode, in the Non-numeric columns . \n",
    "    weather_data = weather_data.withColumn(i,when(col(i) == \"NA\",weather_data.groupBy(i).count().orderBy(desc('count')).collect()[0][i]).otherwise(col(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_data.printSchema() # Using the printSchema() function to print the schema of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Missing Value imputations** \n",
    "\n",
    "The number of Nulls after imputing the missing values are : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls after imputation  in column MinTemp are :  0\n",
      "Number of nulls after imputation  in column MaxTemp are :  0\n",
      "Number of nulls after imputation  in column Rainfall are :  0\n",
      "Number of nulls after imputation  in column WindGustDir are :  0\n",
      "Number of nulls after imputation  in column WindGustSpeed are :  0\n",
      "Number of nulls after imputation  in column WindDir9am are :  0\n",
      "Number of nulls after imputation  in column WindDir3pm are :  0\n",
      "Number of nulls after imputation  in column WindSpeed9am are :  0\n",
      "Number of nulls after imputation  in column WindSpeed3pm are :  0\n",
      "Number of nulls after imputation  in column Humidity9am are :  0\n",
      "Number of nulls after imputation  in column Humidity3pm are :  0\n",
      "Number of nulls after imputation  in column Pressure9am are :  0\n",
      "Number of nulls after imputation  in column Pressure3pm are :  0\n",
      "Number of nulls after imputation  in column RainToday are :  0\n",
      "Number of nulls after imputation  in column RainTomorrow are :  0\n"
     ]
    }
   ],
   "source": [
    "for column in weather_data.columns:\n",
    "    # Selecting Null Values by equating each column to NA \n",
    "    # Using the .count() function to count the number of entries\n",
    "    Null_values = weather_data[weather_data[column] == \"NA\"].count()\n",
    "    # Printing the number of values by using typecasting for the NullValues as they are integer type\n",
    "    print(\"Number of nulls after imputation  in column \" + str(column) +  \" are :  \" + str(Null_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 06: Data transformation\n",
    "\n",
    "#### Converting Numeric Columns to Double :\n",
    "\n",
    "The numeric columns of the dataframe are casted to double using the .cast() function . The 'double' key words indicates the double type.\n",
    "\n",
    "The .cast() function is used to cast the dataframe columns from one type to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical_columns: #Iterating through the numeric column list\n",
    "    weather_data = weather_data.withColumn(i,col(i).cast(\"double\")) # using cast function to convert the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_data.printSchema() # print Schema to check the type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Non-Numeric to double \n",
    "\n",
    "String Indexer is a feature Estimator that  encodes a column of Lables to a Column of Label indicies. The String indexer model can be generated with the StringIndexer() function.This function takes two arguments inputCol i.e. the column to be indexed and the outputCol , the name of the output Column.\n",
    "\n",
    "The .fit() method is used to fit the dataframe and produce a Transformer i.e. the string indexer model is trained on the dataframe df. The Transformer is implemented using the .transform() function inorder to obtained the indexed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in non_numeric_columns:  # iterating through non numeric columns\n",
    "    # building a string indexer model\n",
    "    indexer = StringIndexer(inputCol = column, outputCol = column+\"index\")\n",
    "#     fiting and transforming the string indexer model on the dataframe \n",
    "    weather_data = indexer.fit(weather_data).transform(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- WindGustDirindex: double (nullable = false)\n",
      " |-- WindDir9amindex: double (nullable = false)\n",
      " |-- WindDir3pmindex: double (nullable = false)\n",
      " |-- RainTodayindex: double (nullable = false)\n",
      " |-- RainTomorrowindex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_data.printSchema() # Print the Schema to check the data type of new columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 07: Create the feature vector and divide the dataset \n",
    "\n",
    "### Removing unecessary columns:\n",
    "\n",
    "All the old columns that are not required are removed i..e the original columns before casting the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- WindGustDirindex: double (nullable = false)\n",
      " |-- WindDir9amindex: double (nullable = false)\n",
      " |-- WindDir3pmindex: double (nullable = false)\n",
      " |-- RainTodayindex: double (nullable = false)\n",
      " |-- RainTomorrowindex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the columns that  are not required are removed using the .select() function of the spark dataframes\n",
    "new_data = weather_data.select([column for column in weather_data.columns if column not in non_numeric_columns])\n",
    "# print the schema after removing the columns\n",
    "new_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Encoding the categorical  colums : \n",
    "\n",
    "\n",
    "The indexed columns are encoded into binary vectors, using the One hot Encoder. \n",
    "\n",
    "One Hot encoder is a transformer that takes in an indexed variable and maps a feature vector to that index variable.\n",
    "\n",
    "#### Methodology : \n",
    "\n",
    "All the categorical columns are stored in a list and then the categorical columns are iterated and passed in \n",
    "the OneHotEncoder Function which accepts 2 arguments,i.e. the input column and the name of the output column.\n",
    "\n",
    "The .transform() method is used to transform the data , by converting the indexed strings into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The categorical columns are stored in a list\n",
    "categorical_columns = ['WindGustDirindex','WindDir9amindex','WindDir3pmindex','RainTodayindex']\n",
    "\n",
    "for column in categorical_columns: # iterating through the categorical columns\n",
    "    encoder=OneHotEncoder(inputCol=column,outputCol = column + \"vec\") # initializing oneHotEncoder transformer\n",
    "    new_data  = encoder.transform(new_data) # applying.transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+----------------+---------------+---------------+--------------+-----------------+-------------------+------------------+------------------+-----------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDirindex|WindDir9amindex|WindDir3pmindex|RainTodayindex|RainTomorrowindex|WindGustDirindexvec|WindDir9amindexvec|WindDir3pmindexvec|RainTodayindexvec|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+----------------+---------------+---------------+--------------+-----------------+-------------------+------------------+------------------+-----------------+\n",
      "|   13.4|   22.9|     0.6|         44.0|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|             0.0|            6.0|            7.0|           0.0|              0.0|     (15,[0],[1.0])|    (15,[6],[1.0])|    (15,[7],[1.0])|    (1,[0],[1.0])|\n",
      "|    7.4|   25.1|     0.0|         44.0|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|             9.0|            9.0|            3.0|           0.0|              0.0|     (15,[9],[1.0])|    (15,[9],[1.0])|    (15,[3],[1.0])|    (1,[0],[1.0])|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+----------------+---------------+---------------+--------------+-----------------+-------------------+------------------+------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show(2) # using show() function to show the first 2 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- WindGustDirindex: double (nullable = false)\n",
      " |-- WindDir9amindex: double (nullable = false)\n",
      " |-- WindDir3pmindex: double (nullable = false)\n",
      " |-- RainTodayindex: double (nullable = false)\n",
      " |-- RainTomorrowindex: double (nullable = false)\n",
      " |-- WindGustDirindexvec: vector (nullable = true)\n",
      " |-- WindDir9amindexvec: vector (nullable = true)\n",
      " |-- WindDir3pmindexvec: vector (nullable = true)\n",
      " |-- RainTodayindexvec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.printSchema() # Printing the schema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature generation using Vector Assembler:\n",
    "\n",
    "The Vector assembler takes as input all the raw features and transforms into a single\n",
    "feature vector.\n",
    "\n",
    "The features are given as a list.\n",
    "\n",
    "The VectorAssembler() function takes in two inputs the inputCols and the outputCol, the input coloumn is the list of columns that are to be combined into a feature.\n",
    "\n",
    "The outputCol is the name of the coloumn that should be generated.\n",
    "\n",
    "\n",
    "The  .transform() method is used to generate the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The assembler input is stored in a list \n",
    "assembler_input =['WindGustDirindexvec', 'WindDir9amindexvec', 'WindDir3pmindexvec', 'RainTodayindexvec', 'MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed','WindSpeed9am', 'WindSpeed3pm',  'Humidity9am','Humidity3pm', 'Pressure9am','Pressure3pm']\n",
    "\n",
    "# Calling the VectorAssembler() function to  build the vector assembler transformer.\n",
    "assembler = VectorAssembler(inputCols = assembler_input, outputCol=\"features\") \n",
    "\n",
    "# using the .transform() function to transform the columns in the dataframe \n",
    "new_data_2  = assembler.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- WindGustDirindex: double (nullable = false)\n",
      " |-- WindDir9amindex: double (nullable = false)\n",
      " |-- WindDir3pmindex: double (nullable = false)\n",
      " |-- RainTodayindex: double (nullable = false)\n",
      " |-- RainTomorrowindex: double (nullable = false)\n",
      " |-- WindGustDirindexvec: vector (nullable = true)\n",
      " |-- WindDir9amindexvec: vector (nullable = true)\n",
      " |-- WindDir3pmindexvec: vector (nullable = true)\n",
      " |-- RainTodayindexvec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data_2.printSchema() # Printing the Schema of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Feature and Label Data Frame \n",
    "\n",
    "Creating a DataFrame with the feature and the labels as the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedCols = ['RainTomorrowindex','features'] # storing features and labels in a  list for Sellecting the features and labels from the daataframe\n",
    "df_model = new_data_2.select(selectedCols) # creating a new dataframe with features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(56,[0,21,37,45,4...|\n",
      "|  0.0|(56,[9,24,33,45,4...|\n",
      "|  0.0|(56,[6,21,33,45,4...|\n",
      "|  0.0|(56,[13,16,40,45,...|\n",
      "|  0.0|(56,[0,25,38,45,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model = df_model.withColumnRenamed('RainTomorrowindex','label') # renaming the label column using the withColoumnRenamed function\n",
    "df_model.show(5) # using the show() function to display the first n rows of the dataframe ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the dataset into train and test \n",
    "\n",
    "The train and test data are divided by using the randomSplit() function which takes in a list that consits of the proportions of the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_model.randomSplit([0.7, 0.3]) #splitting the train test using the randomSplit() function.\n",
    "# taking 70 percent of the data for the training set and 30 percent of the day for testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Apply Machine Learning Algorithms\n",
    "\n",
    "\n",
    "## Step 08: Apply machine learning classification algorithms on the dataset and compare their accuracy. Plot the accuracy as bar graph.\n",
    "\n",
    "\n",
    "\n",
    "### a.)  Logistic Regression : \n",
    "\n",
    "In statistics, the logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick. The logistic Regression model can be built by using the LogisticRegression() function.The logistic regression function takes in 3 arguments, the features column, the label column and the maximum number of iterations. The model is trained on the training data, using the .fit() function.The predictions are done using the .transform() function , on the test data.The evaluator used for evaluating the accuracy is the Binary Classification Evaluator.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for Logistic Regression :  80.1898000144913\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building  a logistic regression model\n",
    "# The logistic regression model takes in 3 arguments, the features, the label and the maximum iterations.\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label',maxIter = 10)\n",
    "# Training the model using the .fit() function on the training data.\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "\n",
    "# Predicting the values using the .transfrom() function on the test data\n",
    "lrpredictions = lrModel.transform(test)\n",
    "\n",
    "\n",
    "# Defining a Binary Classification Evaluator to compute the accuracy of the model \n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "\n",
    "# Calculating the accuracy interms of percentage\n",
    "#i.e. multiplying by 100 \n",
    "accuracy_logistic  = (evaluator.evaluate(lrpredictions))*100\n",
    "\n",
    "# printing the accuracy \n",
    "print(\"accuracy for Logistic Regression : \" ,accuracy_logistic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)  Decision Tree \n",
    "\n",
    "\n",
    "A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility.The DecisionTree model can be built by using the DecisionTreeClasifier() function. The DecisionTreeClasifier function takes in 3 arguments, the features column, the label column and the maximum depth. The maximum Depth is How deeper you allow the tree to go to . \n",
    "\n",
    "\n",
    "The model is trained on the training data, using the .fit() function.The predictions are done using the .transform() function , on the test data.The evaluator used for evaluating the accuracy is the Binary Classification Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for decisiontree : 78.22970999715268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Building a decision tree model with the features and labels and setting the maximum depth to 30. \n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 30)\n",
    "\n",
    "#training the model with the train data\n",
    "dtModel = dt.fit(train)\n",
    "\n",
    "#Use the  Transformer.transform() method to predict test data\n",
    "predictions_decision_tree = dtModel.transform(test)\n",
    "\n",
    "\n",
    "# Defining an evaluator to predict the accuracy of the model \n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "\n",
    "\n",
    "# computing the accuracy of the model by using the binary class evaluator, and representing the accuracy interms of percentages\n",
    "accuracy_decisiontree = evaluator.evaluate(predictions_decision_tree) * 100 \n",
    "\n",
    "# Printing the Accuracy \n",
    "print(\"Accuracy for decisiontree :\" ,accuracy_decisiontree)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Random Forest : \n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees. The Random Forest  model can be built by using the RandomForestClasifier() function.The RandomForestClasifier function takes in 3 arguments, the features column, the label column and the numTrees which represents the Number of Trees.\n",
    "\n",
    "\n",
    "The model is trained on the training data, using the .fit() function.The predictions are done using the .transform() function , on the test data. The evaluator used for evaluating the accuracy is the Binary Classification Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of random forest is : \n",
      "82.52060697860145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Building a Random Forest Model using RandomForestClassifier()\n",
    "rf = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\", numTrees=10,maxDepth = 25)\n",
    "rf_model = rf.fit(train) # training the model using fit() function\n",
    "\n",
    "predictions_rf = rf_model.transform(test) # predicting the values of the test data using Transformer.transform() function.\n",
    "\n",
    "\n",
    "# Defining the Binary Classification evaluator to evaluate the accuracy.\n",
    "evaluator =  MulticlassClassificationEvaluator()\n",
    "\n",
    "\n",
    "# Computing the accuracy of the predicted model and multiplying by 100 inorder to represent the accuracy as percentages.\n",
    "accuracy_randomforest = (evaluator.evaluate(predictions_rf))*100\n",
    "print(\"accuracy of random forest is : \")\n",
    "print(accuracy_randomforest) # Printing the accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)  GBT \n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion. The Gradient boosting model can be built by using the GBTClasifier() function.The GBTClasifier function takes in 3 arguments, the features column, the label column and the maxIter , which represents the maximum Iterations . \n",
    "\n",
    "The model is trained on the training data, using the .fit() function.The predictions are done using the .transform() function , on the test data.The evaluator used for evaluating the accuracy is the Multi Classification Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GBT :  82.28644965554273\n"
     ]
    }
   ],
   "source": [
    "# Building the GBT Model()\n",
    "gbt = GBTClassifier(labelCol = 'label',featuresCol = 'features',maxIter=10)\n",
    "# Training the GBT model on train data\n",
    "gbtModel = gbt.fit(train)\n",
    "# Predicting the values of test data using Transformer.transform() function\n",
    "predictions_gbt = gbtModel.transform(test)\n",
    "\n",
    "#Defining a BinaryClassificationevaluator to predict the accuracy\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "\n",
    "#  Computing the accuracy using the evaluator and multiplying into 100 to represent in terms of percentages\n",
    "accuracy_gbt = (evaluator.evaluate(predictions_gbt))* 100 \n",
    "\n",
    "# Printing the accuracy for gbt \n",
    "\n",
    "print(\"Accuracy for GBT :  \" + str(accuracy_gbt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot for Accuracies : \n",
    "\n",
    "A barplot is used to plot the accuracies.\n",
    "\n",
    "All the accuracies are stored in a list and are passed in the y axis.\n",
    "\n",
    "The corresponding model names are stored in a list and are passed in the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAGZCAYAAABYJD8XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZxddX3/8debsAgMi2wBGQSUxSpWJRF3TUDcFWxxwQ2VSmutW01/WqtFWmyljlr3uotrxK1QFxSpwa0uCe6igCwSDKIQlEEFgc/vj3NGr+MkGSD3nMmd1/PxmEfmnHPvuZ9Jvrnzvt/zPd9vqgpJkiQN32Z9FyBJkjRfGLwkSZI6YvCSJEnqiMFLkiSpIwYvSZKkjhi8JEmSOmLwknSLJNknSSXZfBaPfWqSL3VR11yuYViS3C/Jj/quQ9K6GbykeSTJRUmuS7LLtP3fbMPTPv1UNrckGUsymeTTfddyU1TVF6vqwL7rkLRuBi9p/rkQOHpqI8mdgW36K2dO+kvgWuDwJLt3+cKz6TmUtOkyeEnzz3uBpwxsHwO8Z/ABSXZI8p4kP09ycZKXJNmsPbYgyUSSXyS5AHj4DM99R5I1SS5NcmKSBdOLSOM1SS5P8qsk301y0EwFJ3laknOSXJ3kgiR/PXBsSZLVSV7QnmtNkqcNHN85yWnta3wduP0s/o6OAf4L+A7wpGm17JXkY+3fzRVJ3jBw7BkDdf4gycHt/kqy38Dj3p3kxGn1vzDJZcC7ktw6ySfa11jbfj8+8PydkrwryU/b4/89eK6Bx90myUfb81yY5DkDxw5JsrL9e/lZklfP4u9F0i1k8JLmn68C2yf5szYQPR5437THvB7YAbgd8ACaoDYVZp4BPAK4G7AYOGrac98NXA/s1z7mQcBfzVDHg4D7Awe0r/VY4Ip11Hx5+5rbt3W8ZirUtHZvz7EncCzwxiS3bo+9EfgtsAfw9PZrnZLsDSwB3t9+PWXg2ALgE8DFwD7t6y1vjz0GeFn7+O2BR63n55lud2AnYG/gOJr35ne127cFfgO8YeDx76XppbwTsBvwmhl+js2A/wG+3dZ5GPC8JA9uH/Ja4LVVtT1NGD1llrVKugUMXtL8NNXrdThwDnDp1IGBMPaPVXV1VV0EvAp4cvuQxwL/WVWXVNWVwL8PPHch8DDgeVV1TVVdThMKHj9DDb8DtgPuAKSqzqmqNTMVW1WfrKofV+Ms4LPA/aad61+q6ndV9SlgEjiw/Vn+Evjntp7vASdv4O/mycB3quoHNKHqTknu1h47BLgN8A/t+X5bVVMD9f8K+I+q+kZb5/lVdfEGXmvKjcDxVXVtVf2mqq6oqo9W1a+r6mrg5TQBmCR7AA8F/qaq1rY/81kznPPuwK5V9S9VdV1VXQC8jT/8W/wO2C/JLlU1WVVfnWWtkm4Bg5c0P70XeALwVKZdZgR2Abag6dWZcjFNrwk0weOSacem7N0+d02Sq5JcBbyFplfmj1TV/9L04rwRuDzJW5NsP1OxSR6a5KtJrmzP+bC2zilXVNX1A9u/BsaAXYHN11PvTJ5C09NFVV0KnEVz6RFgL+Diaa/FwLEfb+Dc6/Lzqvrt1EaSbZK8pb3M+yvgC8CObZDcC7iyqtZu4Jx7A7eZ+ndo/95eDCxsjx9L09v4wyTfSPKIm1m7pJvA4CXNQ21PzIU0AeZj0w7/gqY3ZO+BfbflD71ia2h++Q8em3IJzaD0Xapqx/Zr+6q60zrqeF1VLQLuSBMC/mH6Y5JsBXwUmAAWVtWOwKeAzOJH/TnNZc911Tv9te4N7A/8Y5LL2jFX9wCe0A56vwS47ToGwF/CuseP/Zo/voFh+oD9mrb9AuBA4B7tpcD7T5XYvs5OSXZc188xUM+FA/8OO1bVdlX1MICqOq+qjqYJxScBH0my7QbOKekWMnhJ89exwKFVdc3gzqq6gWa8z8uTbNeOefp7/jAO7BTgOUnG23FULxp47hqay4CvSrJ9ks2S3D7JA6a/eJK7J7lHki2Aa2jGYd04Q51bAlvRhqgkD6UZH7ZB7c/yMeBlbS/SHflD79VMjgHOoAmCd22/DgK2prm893Wa4PmKJNsmuVWS+7TPfTuwLMmi9saB/dq/O4Bv0YS3BUkeQnvZcD22oxnXdVWSnYDjB36mNcCngTe1g/C3SHL/Gc7xdeDqdtD+1u1rH5Tk7gBJnpRk16q6Ebiqfc5Mf/+SNiKDlzRPtWOmVq7j8LNpwtAFwJeADwDvbI+9DfgMzaDts/nTHrOn0ISlHwBrgY/QDGyfbvv2XGtpLv9dAbxyhjqvBp5DE/jW0lwiPW02P2Pr72guO15GM/D/XTM9KMmtaMavvb6qLhv4upDm0uwxbZB7JM2NAz8BVgOPa+v8MM1YrA8AVwP/TTNgHuC57fOuAp7YHluf/6QJe7+guRni9GnHn0zTK/lDmhsPnjf9BG2tj6AJjxe253o7zU0IAA8Bvp9kkmag/eOr6jcbqEvSLZSq6T3ckiRJGgZ7vCRJkjpi8JIkSeqIwUuSJKkjBi9JkqSOGLwkSZI6MtMkgHPOLrvsUvvss0/fZWySrrnmGrbd1jkR9Qe2Cc3EdqHpbBM336pVq35RVbvOdGyTCF777LMPK1eua7ohrc+KFStYsmRJ32VoDrFNaCa2C01nm7j5kqxzaTIvNUqSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkc2ibUaJUmay1asSN8lbHSTkxOsWLG07zI2qiVLqu8SDF7S+mT03kuZmIClo/VeSnX4XpoTRrBRABMHTLD0hNFqGHV8/79kpem81ChJktQRg5ckSVJHhhq8kjw/yfeTfC/JB5PcKsm+Sb6W5PwkH0qy5TBrkCRJmiuGFryS7Ak8B1hcVQcBC4DHAycBr6mq/YC1wLHDqkGSJGkuGfalxs2BrZNsDmwDrAEOBT7SHj8ZOHLINUiSJM0JQwteVXUpMAH8hCZw/RJYBVxVVde3D1sN7DmsGm6SZDS/Vq3qv4aN/SVJ0iYqNaT7sJPcGvgo8DjgKuDDND1dL2svM5JkL+DT7aXI6c8/DjgOYOHChYuWL18+lDp/b9Wq4Z6/J5Pj44ytXt13GRvXokWdvdQoNovx8UlWrx7ru4yNqsMmwao1I9gogPGtxll97Wi9Vyzao7uGMTk5eu3ihhvGWbBgtNrE2Fg3bWLp0qWrqmrxTMeGGbweAzykqo5tt58C3At4DLB7VV2f5F40QezB6zvX4sWLa+XKlUOpc6Dg4Z6/JysmJliybFnfZWxcHU7aNIrNYmJiBcuWLem7jI3KebxuuYkDJlh27mi9V3Q5j9eoTqA6NjZabaKrCVSTrDN4DXOM10+AeybZJkmAw4AfAJ8Hjmofcwxw6hBrkCRJmjOGOcbrazSXFs8Gvtu+1luBFwJ/n+R8YGfgHcOqQZIkaS4Z6pJBVXU8cPy03RcAhwzzdSVJkuYiZ66XJEnqiMFLkiSpIwYvSZKkjhi8JEmSOmLwkiRJ6ojBS5IkqSMGL0mSpI4YvCRJkjpi8JIkSeqIwUuSJKkjBi9JkqSOGLwkSZI6YvCSJEnqiMFLkiSpIwYvSZKkjhi8JEmSOmLwkiRJ6ojBS5IkqSMGL0mSpI4YvCRJkjpi8JIkSeqIwUuSJKkjQwteSQ5M8q2Br18leV6SnZKckeS89s9bD6sGSZKkuWRowauqflRVd62quwKLgF8DHwdeBJxZVfsDZ7bbkiRJI6+rS42HAT+uqouBI4CT2/0nA0d2VIMkSVKvugpejwc+2H6/sKrWtN9fBizsqAZJkqRepaqG+wLJlsBPgTtV1c+SXFVVOw4cX1tVfzLOK8lxwHEACxcuXLR8+fKh1smqVcM9f08mx8cZW7267zI2rkWLOnupUWwW4+OTrF491ncZG1WHTYJVa0awUQDjW42z+trReq9YtEd3DWNycvTaxQ03jLNgwWi1ibGxbtrE0qVLV1XV4pmOdRG8jgCeVVUPard/BCypqjVJ9gBWVNWB6zvH4sWLa+XKlUOtk2S45+/JiokJlixb1ncZG9eQ2+ygUWwWExMrWLZsSd9lbFQdNglywgg2CmDigAmWnTta7xV1fHcNY8WK0WsXk5MTjI2NVptYsqSbNpFkncGri0uNR/OHy4wApwHHtN8fA5zaQQ2SJEm9G2rwSrItcDjwsYHdrwAOT3Ie8MB2W5IkaeRtPsyTV9U1wM7T9l1Bc5ejJEnSvOLM9ZIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1JGhBq8kOyb5SJIfJjknyb2S7JTkjCTntX/eepg1SJIkzRXD7vF6LXB6Vd0BuAtwDvAi4Myq2h84s92WJEkaeUMLXkl2AO4PvAOgqq6rqquAI4CT24edDBw5rBokSZLmkmH2eO0L/Bx4V5JvJnl7km2BhVW1pn3MZcDCIdYgSZI0Z6SqhnPiZDHwVeA+VfW1JK8FfgU8u6p2HHjc2qr6k3FeSY4DjgNYuHDhouXLlw+lzt9btWq45+/J5Pg4Y6tX913GxrVoUWcvNYrNYnx8ktWrx/ouY6PqsEmwas0INgpgfKtxVl87Wu8Vi/bormFMTo5eu7jhhnEWLBitNjE21k2bWLp06aqqWjzTsWEGr92Br1bVPu32/WjGc+0HLKmqNUn2AFZU1YHrO9fixYtr5cqVQ6lzoODhnr8nKyYmWLJsWd9lbFxDarMzGcVmMTGxgmXLlvRdxkbVYZMgJ4xgowAmDphg2bmj9V5Rx3fXMFasGL12MTk5wdjYaLWJJUu6aRNJ1hm8hnapsaouAy5JMhWqDgN+AJwGHNPuOwY4dVg1SJIkzSWbD/n8zwben2RL4ALgaTRh75QkxwIXA48dcg2SJElzwlCDV1V9C5ipq+2wYb6uJEnSXOTM9ZIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdWTzYZ48yUXA1cANwPVVtTjJTsCHgH2Ai4DHVtXaYdYhSZI0F3TR47W0qu5aVYvb7RcBZ1bV/sCZ7bYkSdLI6+NS4xHAye33JwNH9lCDJElS54YdvAr4bJJVSY5r9y2sqjXt95cBC4dcgyRJ0pyQqhreyZM9q+rSJLsBZwDPBk6rqh0HHrO2qm49w3OPA44DWLhw4aLly5cPrU4AVq0a7vl7Mjk+ztjq1X2XsXEtWtTZS41isxgfn2T16rG+y9ioOmwSrFozgo0CGN9qnNXXjtZ7xaI9umsYk5Oj1y5uuGGcBQtGq02MjXXTJpYuXbpqYIjVHxlq8PqjF0peBkwCzwCWVNWaJHsAK6rqwPU9d/HixbVy5cphFzjc8/dkxcQES5Yt67uMjaujNguj2SwmJlawbNmSvsvYqDpsEuSEEWwUwMQBEyw7d7TeK+r47hrGihWj1y4mJycYGxutNrFkSWeZZ53Ba2iXGpNsm2S7qe+BBwHfA04Djmkfdgxw6rBqkCRJmkuGOZ3EQuDjaboMNgc+UFWnJ/kGcEqSY4GLgccOsQZJkqQ5Y2jBq6ouAO4yw/4rgMOG9bqSJElzlTPXS5IkdWRWwSvJYwbGa70kyceSHDzc0iRJkkbLbHu8XlpVVye5L/BA4B3Am4dXliRJ0uiZbfC6of3z4cBbq+qTwJbDKUmSJGk0zTZ4XZrkLcDjgE8l2eomPFeSJEnMPjw9FvgM8OCqugrYCfiHoVUlSZI0gtY7nUSSnQY2VwzsuxYY8lTykiRJo2VD83itolnoeqa1EAq43UavSJIkaUStN3hV1b5dFSJJkjTqZjuPV5I8KclL2+3bJjlkuKVJkiSNltkOrn8TcC/gCe321cAbh1KRJEnSiJrtWo33qKqDk3wToKrWJnEeL0mSpJtgtj1ev0uygGZAPUl2BW4cWlWSJEkjaLbB63XAx4Hdkrwc+BLwb0OrSpIkaQTN6lJjVb0/ySrgMJqpJY6sqnOGWpkkSdKIuSkTqF4OfHDwWFVdOazCJEmSRs1NmUD1tsDa9vsdgZ8AzvMlSZI0S+sd41VV+1bV7YDPAY+sql2qamfgEcBnuyhQkiRpVMx2cP09q+pTUxtV9Wng3sMpSZIkaTTNdh6vnyZ5CfC+dvuJwE+HU5IkSdJomm2P19HArjRTSnwc2K3dJ0mSpFma7XQSVwLPTbJds1mTwy1LkiRp9Mx2kew7t8sFfQ/4fpJVSQ4abmmSJEmjZbaXGt8C/H1V7V1VewMvAN46mycmWZDkm0k+0W7vm+RrSc5P8iHXfJQkSfPFbIPXtlX1+amNqloBbDvL5z4XGJzl/iTgNVW1H828YMfO8jySJEmbtNkGrwuSvDTJPu3XS4ALNvSkJOPAw4G3t9sBDgU+0j7kZODIm162JEnSpidVteEHJbcGTgDu2+76IvCyqlq7ged9BPh3YDtgGfBU4KttbxdJ9gI+XVV/Ml4syXHAcQALFy5ctHz58ln+SDfTqlXDPX9PJsfHGVu9uu8yNq5Fizp7qVFsFuPjk6xePdZ3GRtVh02CVWtGsFEA41uNs/ra0XqvWLRHdw1jcnL02sUNN4yzYMFotYmxsW7axNKlS1dV1eKZjs0qeN0cSR4BPKyq/jbJEm5i8Bq0ePHiWrly5VDqHCh4uOfvyYqJCZYsW9Z3GRvXkNrsTEaxWUxMrGDZsiV9l7FRddgkyAkj2CiAiQMmWHbuaL1X1PHdNYwVK0avXUxOTjA2NlptYsmSbtpEknUGrw0tkn3a+o5X1aPWc/g+wKOSPAy4FbA98FpgxySbV9X1wDhw6fpeQ5IkaVRsaB6vewGXAB8EvkazQPasVNU/Av8IMNXjVVVPTPJh4ChgOXAMcOpNL1uSJGnTs6HB9bsDLwYOoumtOhz4RVWdVVVn3czXfCHw90nOB3YG3nEzzyNJkrRJWW+PV1XdAJwOnJ5kK5plglYkOaGq3jDbF2mnn1jRfn8BcMjNLViSJGlTtcElg9rA9XCa0LUP8Dqa9RolSZJ0E2xocP17aC4zfgo4oaq+10lVkiRJI2hDPV5PAq6hmX3+OfnDvfWhWSx7+yHWJkmSNFI2NMZrtjPbS5IkaQMMVpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1ZGjBK8mtknw9ybeTfD/JCe3+fZN8Lcn5ST6UZMth1SBJkjSXDLPH61rg0Kq6C3BX4CFJ7gmcBLymqvYD1gLHDrEGSZKkOWNowasak+3mFu1XAYcCH2n3nwwcOawaJEmS5pKhjvFKsiDJt4DLgTOAHwNXVdX17UNWA3sOswZJkqS5IlU1/BdJdgQ+DrwUeHd7mZEkewGfrqqDZnjOccBxAAsXLly0fPny4Ra5atVwz9+TyfFxxlav7ruMjWvRos5eahSbxfj4JKtXj/VdxkbVYZNg1ZoRbBTA+FbjrL52tN4rFu3RXcOYnBy9dnHDDeMsWDBabWJsrJs2sXTp0lVVtXimY50EL4Ak/wz8BnghsHtVXZ/kXsDLqurB63vu4sWLa+XKlcMucLjn78mKiQmWLFvWdxkbV0dtFkazWUxMrGDZsiV9l7FRddgkyAkj2CiAiQMmWHbuaL1X1PHdNYwVK0avXUxOTjA2NlptYsmSzjLPOoPXMO9q3LXt6SLJ1sDhwDnA54Gj2ocdA5w6rBokSZLmks2HeO49gJOTLKAJeKdU1SeS/ABYnuRE4JvAO4ZYgyRJ0pwxtOBVVd8B7jbD/guAQ4b1upIkSXOVM9dLkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktSRoQWvJHsl+XySHyT5fpLntvt3SnJGkvPaP289rBokSZLmkmH2eF0PvKCq7gjcE3hWkjsCLwLOrKr9gTPbbUmSpJE3tOBVVWuq6uz2+6uBc4A9gSOAk9uHnQwcOawaJEmS5pJOxngl2Qe4G/A1YGFVrWkPXQYs7KIGSZKkvqWqhvsCyRhwFvDyqvpYkquqaseB42ur6k/GeSU5DjgOYOHChYuWL18+1DpZtWq45+/J5Pg4Y6tX913GxrVoUWcvNYrNYnx8ktWrx/ouY6PqsEmwas0INgpgfKtxVl87Wu8Vi/bormFMTo5eu7jhhnEWLBitNjE21k2bWLp06aqqWjzTsaEGryRbAJ8APlNVr273/QhYUlVrkuwBrKiqA9d3nsWLF9fKlSuHVmdb7HDP35MVExMsWbas7zI2riF/WBg0is1iYmIFy5Yt6buMjarDJkFOGMFGAUwcMMGyc0frvaKO765hrFgxeu1icnKCsbHRahNLlnTTJpKsM3gN867GAO8AzpkKXa3TgGPa748BTh1WDZIkSXPJ5kM8932AJwPfTfKtdt+LgVcApyQ5FrgYeOwQa5AkSZozhha8qupLwLr6Xg8b1utKkiTNVc5cL0mS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHRla8EryziSXJ/newL6dkpyR5Lz2z1sP6/UlSZLmmmH2eL0beMi0fS8Czqyq/YEz221JkqR5YWjBq6q+AFw5bfcRwMnt9ycDRw7r9SVJkuaaVNXwTp7sA3yiqg5qt6+qqh3b7wOsndqe4bnHAccBLFy4cNHy5cuHVicAq1YN9/w9mRwfZ2z16r7L2LgWLerspUaxWYyPT7J69VjfZWxUHTYJVq0ZwUYBjG81zuprR+u9YtEe3TWMycnRaxc33DDOggWj1SbGxrppE0uXLl1VVYtnOtZb8Gq311bVBsd5LV68uFauXDm0Ottihnv+nqyYmGDJsmV9l7FxDbHNTjeKzWJiYgXLli3pu4yNqsMmQU4YwUYBTBwwwbJzR+u9oo7vrmGsWDF67WJycoKxsdFqE0uWdNMmkqwzeHV9V+PPkuwB0P55ecevL0mS1Juug9dpwDHt98cAp3b8+pIkSb0Z5nQSHwT+DzgwyeokxwKvAA5Pch7wwHZbkiRpXth8WCeuqqPXceiwYb2mJEnSXObM9ZIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdcTgJUmS1BGDlyRJUkcMXpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSRwxekiRJHTF4SZIkdaSX4JXkIUl+lOT8JC/qowZJkqSudR68kiwA3gg8FLgjcHSSO3ZdhyRJUtf66PE6BDi/qi6oquuA5cARPdQhSZLUqT6C157AJQPbq9t9kiRJIy1V1e0LJkcBD6mqv2q3nwzco6r+btrjjgOOazcPBH7UaaGjYxfgF30XoTnFNqGZ2C40nW3i5tu7qnad6cDmXVcCXArsNbA93u77I1X1VuCtXRU1qpKsrKrFfdehucM2oZnYLjSdbWI4+rjU+A1g/yT7JtkSeDxwWg91SJIkdarzHq+quj7J3wGfARYA76yq73ddhyRJUtf6uNRIVX0K+FQfrz0PeblW09kmNBPbhaazTQxB54PrJUmS5iuXDJIkSeqIwUuSJG1USTZLkr7rmIsMXtI8kmSLJI/yDVHSMCTZo/32L4G791nLXNXL4HpJvTkR+G1VOYWLbpYkm1XVje0au1sDO1XVGX3Xpf4l2RV4aJK7AI8ADmj3L6iqG3otbg6xx0sb1C5sTpKdk9whydK+a9JNl+QA4HDgX9rtBf1WpE3NQOi6E/BfwKOBE5PsZy+qgGuA84CjgZ8DRyXZfSp0JTmoz+LmCu9q1KwlWQFcDmwBXAu8rqq+0mtRmrUkpwH3Aw6rqrPbfSnfBHQTJfk08EqaHo0HVtVRSQ4Etqiq7/Vbnfow2KuV5LE083QuBrYCPgbcB7iuqk7qr8q5wUuNWq+p/0xJDgYuA54I7A0cARyfZA3wPOCX/gKfu5I8EtgWWAZ8IMm3gJdW1XntcQOYZiXJNsB3gHOB44FntYf+FrgQMHjNQwOh6+3Ai4C1wA+A+9P0gN0XeGBvBc4hXmrUeg1cl38x8ON2+0LgzcALadbZvNZf2nPekcB/VNU7gEXAT4HPJHl5kjH//TRbVfVr4HzgW8CVVfW9JPvS/IJ9b6/FqRdJNmv/fB6wS1X9gqZjZ0vgVOA5wNKq+pN1mecjLzVqvdpxQFvSzGB8P+BtwCsGPt1sUVW/mxr70WOpWo8ki6pq1bR9+wOvAO4J/HVVfaKX4jTnDYzt2ha4DtgFeBRwL+DPaT6M/V9VTfRYpnqUZAvgLJqrIbcDngo8FjgFeF5VXdtfdXOLwUszmilIJbk98FpgITBRVR/qpTjNWpLtaC4vBji3qt43w2OOBM53bI5mMhC6dgZeD1wBbAO8AbgK2Bf4YVX9tMcy1bMkWwGvAcaA3YE3VNVpST5HE7x8f2kZvDSjqTE/bdfxHWi6jb8JvInmstUbgYdV1bd6LNI9kf4AABb7SURBVFMbkOQNNEH5/4DbAycAhwCfq6rf9lmbNi1JPgB8AdiBZkD94e2cTWttS/PT9A/o7Qe9xwOrqursJI+j6U0/tLci5yAH1+tPDHzCXQI8ATiJ5vLCUuDVVfV84OM9lqhZaG+IWFxV92y3z6P5JFrAvyV5blV9vs8atWlIshPNB/X/SvLfwH+2h54CrAHe01tx6s1U6EryT8B2wH7AcVV1ZZJ9gOfzh5sv1HJwvf7EwCeYw4G3V9VHgc8Bbwdum+R+0PSK9VSiZuevgZ2SbNf+m/0MOIbml+W7aXq+pA2qqiuBVUnObrc/2Y7/fBKwar1P1kgaGFD/aJobKz7Q/rlDkq2q6iLgydPHlsrgpWmS3G7qPxTwdeBvkxxUVb+pqh8AvwYOBPBOuLmrHQT9TpoxF++jubPov6pqsr0r7WfAg3osUXPcwMTJf5XkJcCHaaaKuCjJSTTt66yq+n6PZaonAx/Qn0Zz1+JDgVOq6kLg3kleQHP3q6bxUqN+r10C5PCqem2Sfavq1HZSxFckuZRmnNA9gWPbxzv309z1YpqBz6fTzLl0CfDoJDtU1RuBo2jG6UkzGphK5kHAK6vq4iRvAfYB/gz4CE370jwz7b3/FOAvgIdX1X3bfc8Cvuzvh5kZvDToh8BlSR4CPDHJcpoZhz9H86lmV+BJVXWda2/NXUleSHOn2d9W1VXtvt2BK4FDk1xEMyfbx/qrUnNZkntX1VeSPBR4APC/wDeq6stJvuIv1Plr4MarnWmGLqyimdPxVu2SQIcDu1XVa/qscy7zrkYBfzRD/WbA9sBxwJ1oZqc+s6q+2muBmpU2YH0BuE9V/bzd91zgJcDFNJcet6f5Nz23t0I1ZyV5BvCIqjqi7QX/S5per1XAW6rqnF4LVK+m3fG+c1W9tJ1K4qU0M9OfBqxwObl1M3jpjyT5O+D0qjo/yZ8Dj6GZhuCbwBvb8UGao5I8lSZ0PaOd0HAL4KPAC4B70PRe/H07WFr6I0m2BL4KPGNqUHSSWwF3Bh4O3JVmxvp/tcd7/mp/N7wO+FBVvXlg/xZV9bv+Kts0OLheg4NoH06zptbUsg43Ai+jmbX+KkPXJuFrwMKpN8D23+xl7Y0RXwJuA1zTa4Way04EdgS2aacDoKp+W1XfoBkT+A7gp4au+au9m/03wO+A57cLYgNg6Jode7z0e0k+QfPG+23gn2guL/yqqh448BgH1M9R7RvirWhu6/458Pqq+u7A8Q/R3IX2pp5K1BzWLiH1YZoe0n1pbshYAXynqq5oH7M5zQ3NBq95Zh2rmRwKvJJm7dcTq+prvRS3iTF4Cfj9G+qJNFNFbE0zoP4dNEsE/WdVnd1jeboJkuwN/A3N0h1X0gTpOwGPrCrn7tKMkrwOuLCqXpPkLjS937sDZwNfpFlyyt7SeWjwA3eSV9GsXrAZcGp79/tLadZlPNherw0zeM1jAzPUb06z9toONMsBXV1V705yT+BNVXVwr4XqJktya5q7ixYD9wbeS3N7t+ulaUZJ9mknvRzc9yCa94StgU9V1Yf7qE39Grj56hXAHjR3uxfNtDRfaVc02MbhKLNj8JrHBoLX62gWuX3TtOOfBU6uqvc7fYQ0fwxeVmoH3B8LfLeqvtRvZepLe+fiR2huvLisnaR5CfBU4NlVdVmP5W1SDF7z1MAtwXekWWft7sBWNMvM7AD8D7Cgqlb2WKakHs00rkfzR3s15ME0vZ2V5D+BvYBjB+YI/ArN+oz2ps+SwWueS/IPwJ40QeuBNLeLfwNYWVWn9VmbpO5N7932hpr5K8m/0dxM8U/t9jY0g+n3pZliaAFwp6p6ZH9VbnqcTkIfAG4A/hP4ZlU9lObOuPv3WpWkoRuYSmabJHsl2XwqdE0dM3TNT0n2o7mz/eXt9rE0V0P+CXgVsDfN2p1P76vGTZU9XvPMtLtT9qSZ02lf4PKqujTJATQzD9+7qq70UoM0+pKcDqylWYPxjVX1tna/vV3zVJKTgW2q6jFJ7g28wRutNg57vOafzeD3y8j8K/BlmoWxL02yA3Ao8B9t6Fpg6JJGU7s8GEmeAFxLM0j6H4GnJTkryYMMXfNTe0nxfGB1kucD/wWc1B7brB37pZvJ4DXPtLcE70wzGPLpwC9o5nqCZt6nd1XVO6ce21OZkoasvaN5a+DPaeZjuraqPl1V96bp9f6nfitUX9ppIf4dOB3Yheb3xFiSO1fVjVV1fa8FbuK81DgPJXkUcAjwGZrZhh/Q3ir8WZpbhV08WZoHktwfeANwOfBC4AdV9Zv2mOvuzVPThqTsADwUWESzjNx5wGlVdXmPJW7S7PGaJwYuK2xHE7BCswbjK9uHPBm40tAljbap9wKAqvoCcE+ano1XAc9Lsn87zMDQNU8NhK5U1S+rajnwFpplpO4ObNlnfZs6e7zmmSQfpblevxB4Bs0b7h2A/Wl6u77vZKnS6EvydGBXmkWx30xzd/OJwG2Bo6pqbY/laY5o73S9fmB7v6o6v8+aNnUOkJsHBmaovy+wZVWd0e6/kGberlOBi9vQtZmhSxpNA0u//CXNbPTvp+m9+B/gVVX1tCSLDV3zz/Q7WJPsWlU/nwpdUwHM0HXL2eM1jyQ5CXgE8M/A/1TVdT2XJKkHSc4ATqqqz7XbDwCeBvx1VV3ba3HqxcAH9GcBDwEuAFYDH6uqH/db3WhxjNeIGxzPQXOn0qdopoz4qyR36qcqSV1LstvA5lk0g6UBqKqzaCbEvG/XdWluGLjL9bHA24Av0UyYuizJM9vxwdoIDF4jrO06vjHJzkleC1xKc+fSmcB+wN8meVivRUoauiR3B5aksQPwceDoJJ9Kcr8kTwS2rqoz+61UfUiS9tu9gP9tl4v7CPA+4Cs0A+pv31N5I8cxXvPDw4EHAAcBZ9DcPv5F4Ck03cmSRtskzY00f0HzS/SdwH2A44DX0dzp/MLeqlNvBi4xHgS8ADiyHQv4z8APk1xEs3bvOb0WOkIc4zWiBv4zPYhmTNe/A1sD96P55PLmqvpknzVK6laSQ4EHAtsCPwQ+VFVXrv9Zmg+SfJjmash3gP+guSL2cn9PbHxeahxRA0v9PBg4uf3P8ymaT7qbAc9I8py+6pPUrXZpoC8ArwG+ChwInJDkKS4BM78luRvNFEMfq6qvVNV9gXcD70vyN70WN4IMXqPva8Czk9y1qn5dVd8GLqL5VHO3JHv0Wp2koUmyoP3z6cBjgBur6ufAfwP/j+a9YNIlYOa9/YEFwNunxv1W1VuB3WgCmDYiLzWOmOlzsbT7/gZ4GHAZsBJ4QVUdmOTLwLOq6ls9lCqpA+3A6bNper9vBJ4NPBOYqKr/mOk9Q6Nvhnm7bgccARxAs4TUp6vqq1PDVvqqcxTZ4zV6As0n3CTPS/Im4MfAy4BraWaq/ov2ssNaQ5c0mtq7FwH2oPnQ9RTgFTQz1D8AWJTk1oau+acNU5VkhyRPTPJemrG/pwOn0IwBfCT80bAVbST2eI2QqU8wSQ4E3kPzJnsizaSIXxp43PbAMuDtVfWTfqqVNCxJDgO2AD7Tvifcj+Ymmw9W1TuTLAH+o6oO6bNO9WPgd8UHgSuAXwCH0Nzl/hKa4HVjVa3pscyRZfAaQUleBfwvsBY4vqoenGR3mjEeb6+q3yTZrqqu7rVQSUOR5K40Pd1H0UyCeUpV/bQ9tiPN3EzPdt6u+WXw8mJ7afEDVXXPdntrmjU7r6uq43osc+R5J8uIaWeq/zJwGHB/4PHtoecBC6vqNwCGLmk0tb9cv9V+/0ua94HbJ/k88G3gV8BLDV3z0hbA1FJxVwEXtOt2nl5V1yR5JnBqkrGqmuytyhFn8BoRSbZp71q8McmZwBNoBtLul2RX4FE0SwX9fo6vHsuVNCQDPRpPby8rfp5mfNcRwMHAmVX10T5rVG9OTPJnNL2dF7Vrdh4FVJIbaCbbPs/QNVxeahwB7eLXO9PcjXJ8VX2+Hed1OHA0zdQRZ1XVckOXNLraGcdvaBc6fgDwhKmpItr3hL8B/q+qTumzTvWjnT7o+cBDgbdW1euTPJhmfcbrgd8AL3Sh9OEyeG3ikjyaZj6eZcDi9uvpVfW7dTzeW8elEZZkC5oFjh/X9mrcqqp+m2T7qvpV3/WpHwOh/C9oernuQnPZ+fiq+my/1c0vBq9NXJKv03xC+Xy7/U6aa/h70wyunwROqqof91elpK4k2Rl4K82KFacN7H8v8Pqq+npvxalXSfYBVgAPAn4L3A34F+BSmt8j3+2rtvnEMV6bsCSLgZ2AxUkuqarzaXq8vkRzd8pdgB8YuqT5o6quSHIacGiStcBPgXsAexu65r3bAV+oqnMBkqxu9z2SZi1fdcAer01ckjsBzwH2Aq4EtquqIwaOT83X4iVGacQl2ZfmzrXNgSfRTCXx5zTh65VVtbLH8tSzdlqhM4BPVtWL2n0vBK6vqlf1Wtw8YvDaRCXZvKquT7JFVf2uHSD5dGAbmpXlz62qn/VbpaRhGxi7cwTN3cz7A98C/pZm1vqraX6xXtVjmZojktwWeCXN1ZH/AR4BHF5VF/Za2Dxi8NoETZsE7z3Aqwfm7XkWzZvv12nWZPQORmkeSPJFmkmSnwnsVlXPTLIfcJGLYM9PA6H8CTRTRVwEfINmgu070CyCfe7UpUd1w+C1CRq4fPgS4E5VdfRUD1h7fGfgwKr6ipcYpdHXDjl4FvAumvGdh1bVr5J8Cni300fMPwO/J7aiGff7HpqlgHalGVj/GeCrVXXdek6jIXBw/SZkWk/XlsAdgbe3hzdr9y8Ffl1VX4E/TKYoaXRV1feTnAu8DXh/G7oOAXYxdM17zwD+p52za0vgXjQ3WxxLMy74e30WNx8ZvDYh7aeXqbl4rkvyFeCoJGdX1dr2Yf8K/FN/VUrqQpLb0yxsfBua6SPOpFke6HZJltP0bLyivwrVp/b3xa1oJtJ+YJIfVNVHgLOSfBc4qKoMXT3wUuMmpB08uxfwnvYT7S7AG4ALaWYcvi2wc1U9uscyJQ1Zkr+nmZm+aC4f/TnNh6630CwNdiPws6r6Tm9Fas5I8hyaZaMuAP65qn7Yc0nzmsFrE5MkND1aTwD+CjgbeDKwC/AL4LSq+tnUoMr+KpU0DEkWAmcBD66qi9t996Pp9TofONq19uavgQH1OwC3B37ULoC9K/Bs4DjgeVW1vNdC5zGD1yYmyR1pZqR/CM3t4t+lWfLhkl4Lk9SJJO8Cvl5Vb06yNXDd1IesJGcCL6uqL/ZapHqX5H9pgvhjaaaPeFNVrU1yZ5o7Xa/utcB5bLO+C9CGJdk5yaOSbEczZmPbqnoXze3BPwU+n8SxHNKISzIOHAPsB1BVv2l7N7ZtH/J14KC+6lO/2isiJHkBcA7N5NpX0Fx+/k6SY4HvGbr6ZfDaNNxIM4j2JzR3KZ0PUFWXV9VLgCNpJsL7/X88SaOnqlYDBwB/nmRNkmPa/de0D7kj4Jxd89TA9BGLaHq5Xg38a1UdBnwbeKJ3uvfP4LUJaO9YnKC57ffaJGckeQhAknsCB1fVl9vH+p9KGmFVdX5VHU4zxvOlSc5Osm+SRcD2VfW2nktUj6rqWppVTC4HxoCpsb6/wDve5wTHeM1xA5Pg7Q/8rqouamenfxrNivJ7A8+tqrN6LVRSL5L8P+BFwI7Aw6rq9J5LUseSPJJmsevtgTOq6qvt/kfT3Ii1DbBVVT2wvyo1xeA1hw3cnfJgmlvF719Vv02yGbAzzaDJ6/yEK81vSbahuYzke8E80y58fSbNMkC/ppli5MVVdWP7u+L+wHU0A+p/2l+lmmLw2gQkOYvmOv3nkvwdzZw9n6+qDw48ZjPXZZSk+aW9y/ULVfWuJLcD/o1mmqG1NGsx/gj4clWt6bFMDXDm+jkuyR40dy4uSPIWmk8zlwAPTvI54BfVMHRJ0jwycJfrLwGq6oIkBTwO+CawALgL8MneitSfMHjNQUnuD9y1ql5XVWuSfINmXa1zq+olSQ4A3ksbunotVpLUi6pa3f4+eHOSS4F3AvtU1aKpxyTZ0oWw5xYvNc5B7TX7AA8D9qyqfxk4tg3w38AH265lZ6iXpHkuySNo5nncDbhHVV3Yc0laB6eTmIOq6rL2evyPgfsk+WKSv2wP7wKc2U6giqFLklRVn6iqg4BXAauSnJ7Eq1pzkD1ec1SS/avqvPb7Y4C/Bn5Gc7fKOe1+B9RLkv5Iu5LBX1fVq/uuRX/K4DWHTAWpJItplv44GXhBVV3ZLhd0AvDbqnpxr4VKkqSbxeA1ByX5N5rLwAcD9wBeXVUntMem5vayt0uSpE2MwWuOaefpenS7thZJDgY+QbPcw7Kq+myf9UmSpJvPwfVzz8XAqqmNqjobeAnwHeDvkuzcV2GSJOmWMXjNPSuBpUnek+TWSbanmcPrDTSD6+/da3WSJOlmM3j1LMmC9s/tktwW2BO4J82lxa8Db6Tp7VoFHAJ8radSJUnSLeQYrzkiySnAtcCBNCsKPAmYWltrEngRsGVVvbSfCiVJ0i3l5GpzQJIjgV2q6tB2+ynA+4GnVNV3k2wBnFJVP+qzTkmSdMsYvOaGbWjGdk2tq/WeJHsC9we+W1W/o1lhXpIkbcIc4zU3fINmQP0zBxYzXQTcCJAkvVUmSZI2Gsd4dSxJqqraNbRuqPYfIMl9gLcA19EMqt+vqh7YY6mSJGkjM3h1LMlmwG5VdVm7vQVNAJvq3TocuAz4SVX9cmqm+v4qliRJG4uXGrt3IHBhkpMAqup37fqM27XHdwQOqqpftscNXZIkjQiDV8eq6hxge+CaJGuSHNPuvzrJbsCbGZi5XpIkjQ4vNfaoXf7nZOA2wJHA84HNquq5LoItSdLoMXjNAUnuAnwa2B3YuqqunRqE33NpkiRpIzJ4zRHtlBG3raqLk2xeVdf3XZMkSdq4DF6SJEkdcXC9JElSRwxekiRJHTF4SZIkdcTgJWlOSnJkkkpyh75rGZTkK33XIGnTZfCSNFcdDXyp/XMokiy4qc+pqnsPoxZJ84PBS9Kck2QMuC9wLPD4gf0vTPLdJN9O8op2335JPtfuOzvJ7ZMsSfKJgee9IclT2+8vSnJSkrOBxyR5RpJvtM//aJJt2sctTPLxdv+3k9y73T85cN5/aJ/7nSQntPu2TfLJ9jnfS/K4of+FSdpkbN53AZI0gyOA06vq3CRXJFkE7Nbuv0dV/TrJTu1j3w+8oqo+nuRWNB8o99rA+a+oqoOhWUGiqt7Wfn8iTdh7PfA64KyqenTbMzY2eIIkDwL2Bw4BApyW5P7ArsBPq+rh7eN2uGV/FZJGicFL0lx0NPDa9vvl7XaAd1XVrwGq6sp2cfk9q+rj7b7fAjTzEa/Xhwa+P6gNXDvShKvPtPsPBZ7SnvcG4JfTzvGg9uub7fYYTRD7IvCqJCcBn6iqL87yZ5Y0Dxi8JM0pbU/WocCdkxSwACjgwzfhNNfzx0MpbjXt+DUD378bOLKqvt1ejlwy21KBf6+qt/zJgeRg4GHAiUnOrKp/meU5JY04x3hJmmuOAt5bVXtX1T5VtRdwIU2P09MGxmDtVFVXA6uTHNnu26o9fjFwx3Z7R+Cw9bzedsCaJFsATxzYfybwzPa8C2a4ZPgZ4OnteDSS7JlktyS3AX5dVe8DXgkcfEv+MiSNFoOXpLnmaODj0/Z9FNgDOA1YmeRbwLL22JOB5yT5DvAVYPequgQ4Bfhe++c3WbeXAl8Dvgz8cGD/c4GlSb4LrALuOPikqvos8AHg/9rHfIQmxN0Z+Hpb4/HAibP/0SWNOtdqlCRJ6og9XpIkSR0xeEmSJHXE4CVJktQRg5ckSVJHDF6SJEkdMXhJkiR1xOAlSZLUEYOXJElSR/4/H6x/60OHCZ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = [accuracy_logistic,accuracy_gbt,accuracy_gbt,accuracy_randomforest]\n",
    "x_axis = [\"LogisticRegression\",\"GBT\",\"Decision Tree\",'Random Forest']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x_axis,accuracies,color = ['r','b','g','y'])\n",
    "\n",
    "plt.title('Models and Accuracies')\n",
    "plt.ylabel('Models')\n",
    "plt.xlabel('Accuracies')\n",
    "plt.xticks(rotation = 60)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 09 \n",
    "### Calculating Confusion Matrix and finding the precision recall and F1 Score of each Classification Algorithm \n",
    "\n",
    "A **confusion matrix** is a table layout that allows the visualization of the performance of an Algorithm.\n",
    "\n",
    "**Recall** is the ratio of the correctly predicted observations to all the observations in actuall class\n",
    "yes. **Recall** is refered to as true positive rate or sensitivity. **Precision** is the positive predictive value \n",
    "The confusion matrix is generated by the MulticlassMetrics().confusionMatrix() function. This function takes in the actual values and predictions as tuples.This can be achieved by converting the predicted dataframe to an RDD and then using map and lambda function to select the prediction and label.The .select() function of the Spark Dataframe is used to select the predictions and labels . \n",
    "\n",
    "\n",
    "The confusion matrix is of the form **[ [ TN, FP], [FN , TP ] ]** , This order is considering label Yes as positive. where FN = False Negative , FP = False positive , TN = True Negative , TP = True Positive. The formulae are : \n",
    "\n",
    "Recall = TP/TP+FN\n",
    "\n",
    "Precision = TP/TP+FP\n",
    "\n",
    "**F1Score = 2*(Recall * Precision)/(Recall + Precision)**\n",
    "\n",
    "Additional functions used for this task: \n",
    "\n",
    "The additional functions used for this task are .map() for the RDD, inorder to select the labels and the predictions.Also, the .toArray() function is used to convert the confusin matrix to NumpyArray\n",
    "\n",
    "#### Confusion Matrix for GBT : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for gbt is : \n",
      " \n",
      "[[31525.  1617.]\n",
      " [ 5321.  4323.]]\n",
      "Precision for GBT : 0.7277777777777777\n",
      "Recall for GBT : 0.448257984238905\n",
      "F1Score for GBT : 0.5547997946611909\n"
     ]
    }
   ],
   "source": [
    "##  Using MulticlassMetrics() .rdd()  and .confusionMatrix() function to generate a confusion matrix\n",
    "\n",
    "metrics_gbt = MulticlassMetrics(predictions_gbt.select('label','prediction').rdd.map(lambda x: (x.prediction,x.label)))\n",
    "confusion_GBT = metrics_gbt.confusionMatrix().toArray()\n",
    "\n",
    "# Generating Precision Recall and F1 Score from  Confusion Matrix : \n",
    "\n",
    "precision_GBT =  confusion_GBT[1][1]/ (confusion_GBT[0][1] + confusion_GBT[1][1])\n",
    "recall_GBT = confusion_GBT[1][1]/ (confusion_GBT[1][0] + confusion_GBT[1][1])\n",
    "F1Score_GBT = 2 * ((precision_GBT * recall_GBT)  / (precision_GBT + recall_GBT))\n",
    "\n",
    "\n",
    "\n",
    "print('confusion matrix for gbt is : ')\n",
    "print(' ')\n",
    "print(confusion_GBT)\n",
    "\n",
    "print('Precision for GBT : '+ str(precision_GBT))\n",
    "print('Recall for GBT : '+ str(recall_GBT))\n",
    "print('F1Score for GBT : '+ str(F1Score_GBT))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for Logistic Regression : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for Logistic regression is \n",
      " \n",
      "[[30882.  2260.]\n",
      " [ 5586.  4058.]]\n",
      "Precision for logistic  0.6422918645140867\n",
      "\n",
      "Recall for logistic :  0.4207797594359187\n",
      " \n",
      "F1 Score for logistic :  0.5084575867685753\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix:\n",
    "\n",
    "##  Using MulticlassMetrics() .rdd()  and .confusionMatrix() function to generate a confusion matrix\n",
    "metrics_logistic = MulticlassMetrics(lrpredictions.select('label','prediction').rdd.map(lambda x: (x.prediction,x.label)))\n",
    "confusion_logistic = metrics_logistic.confusionMatrix().toArray()\n",
    "\n",
    "# Generating Precision Recall and F1 Score from  Confusion Matrix : \n",
    "precision_logistic=  confusion_logistic[1][1]/ (confusion_logistic[0][1] + confusion_logistic[1][1])\n",
    "recall_logistic = confusion_logistic[1][1]/ (confusion_logistic[1][0] + confusion_logistic[1][1])\n",
    "F1Score_logistic = 2 * ((precision_logistic * recall_logistic)  / (precision_logistic + recall_logistic))\n",
    "\n",
    "\n",
    "print('confusion matrix for Logistic regression is ')\n",
    "print(' ')\n",
    "print(confusion_logistic)\n",
    "\n",
    "\n",
    "print(\"Precision for logistic \",precision_logistic)\n",
    "print(\"\")\n",
    "print(\"Recall for logistic : \",recall_logistic)\n",
    "print(\" \")\n",
    "print(\"F1 Score for logistic : \",F1Score_logistic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix for Random forest  : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for Random Forest \n",
      "[[31166.  1976.]\n",
      " [ 5005.  4639.]]\n",
      "Precision for rf  0.7012849584278156\n",
      "\n",
      "Recall for rf :  0.4810244711737868\n",
      " \n",
      "F1 Score for rf :  0.5706378006027432\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix:\n",
    "##  Using MulticlassMetrics() .rdd()  and .confusionMatrix() function to generate a confusion matrix and the .toArray() function is used to convert the confusion matrix to a NumpyArray()\n",
    "Metrics_rf = MulticlassMetrics(predictions_rf.select('label','prediction').rdd.map(lambda x: (x.prediction,x.label)))\n",
    "confusion_rf = Metrics_rf.confusionMatrix().toArray() \n",
    "\n",
    "# Generating Precision Recall and F1 Score from  Confusion Matrix : \n",
    "precision_rf=  confusion_rf[1][1]/ (confusion_rf[0][1] + confusion_rf[1][1])\n",
    "recall_rf = confusion_rf[1][1]/ (confusion_rf[1][0] + confusion_rf[1][1])\n",
    "F1Score_rf = 2 * ((precision_rf * recall_rf)  / (precision_rf + recall_rf))\n",
    "\n",
    "print('confusion matrix for Random Forest ')\n",
    "print(confusion_rf)\n",
    "\n",
    "\n",
    "print(\"Precision for rf \",precision_rf)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Recall for rf : \",recall_rf)\n",
    "print(\" \")\n",
    "print(\"F1 Score for rf : \",F1Score_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix for Decision Tree : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix for decision tree :\n",
      " \n",
      "[[28456.  4686.]\n",
      " [ 4637.  5007.]]\n",
      "Precision for decision_tree  0.5165583410708758\n",
      " \n",
      "Recall for decision_tree :  0.5191829116549149\n",
      "\n",
      "F1 Score for decision_tree :  0.517867301029115\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix:\n",
    "\n",
    "##  Using MulticlassMetrics() .rdd()  and .confusionMatrix() function to generate a confusion matrix\n",
    "\n",
    "\n",
    "confusion_decision_tree = MulticlassMetrics(predictions_decision_tree.select('label','prediction').rdd.map(lambda x: (x.prediction,x.label))).confusionMatrix().toArray()\n",
    "\n",
    "# Computing Precision Recall and F1 Score from the confusion matrix : \n",
    "precision_decision_tree=  confusion_decision_tree[1][1]/ (confusion_decision_tree[0][1] + confusion_decision_tree[1][1])\n",
    "recall_decision_tree = confusion_decision_tree[1][1]/ (confusion_decision_tree[1][0] + confusion_decision_tree[1][1])\n",
    "F1Score_decision_tree = 2 * ((precision_decision_tree * recall_decision_tree)  / (precision_decision_tree + recall_decision_tree))\n",
    "print('confusion matrix for decision tree :')\n",
    "print(' ')\n",
    "print(confusion_decision_tree)\n",
    "\n",
    "\n",
    "print(\"Precision for decision_tree \",precision_decision_tree)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Recall for decision_tree : \",recall_decision_tree)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"F1 Score for decision_tree : \",F1Score_decision_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How the Accuracy can be Improved** ?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision can be seen as a measure of exactness (quality) and recall is measure of completness(quantity). The precision is higher than recall for Logistic Regression, Random Forest and GBT , it indicates that the algorithm generates more relavant results than irrelavant results.The recall is higher than precision for Decision tree,it indicates that the algorithm returned most relavant results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the prediction can be improved by:\n",
    "    \n",
    "1.) Even though the missing values were imputed, the Outliers should also be detected in the Numeric columns : \n",
    "    'MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm','Pressure9am' and  'Pressure3pm'. The presence of outliers leads to a biased model. \n",
    "        \n",
    "    \n",
    "    \n",
    " 2.) Feature Engineering : Changing the scale of variables from original scale to scale between zero and one . i.e. Data Normalization. Data Should be normalized for columns such as Pressure as the values are much higher than other values.\n",
    "    \n",
    " \n",
    "        \n",
    " 3.) Feature Selection : Finding the best subset of attributes which explains the relationship between the independent variables and the target variables . The features that have the higher correlation with the label should be slected in order to improve accuracy of the models. \n",
    "\n",
    "    \n",
    "4.) Essemble methods such as Bagging and Bosting which combines multiple weak models and produce much better results.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
